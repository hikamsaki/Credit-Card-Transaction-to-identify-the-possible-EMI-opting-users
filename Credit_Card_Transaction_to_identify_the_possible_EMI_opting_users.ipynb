{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data from Google Drive\n",
        "data = pd.read_csv('/content/drive/MyDrive/case_study_devdata.csv')\n",
        "\n",
        "# Data Processing\n",
        "data.drop(data.iloc[:, 8:288], axis=1, inplace=True)\n",
        "data = data.fillna(0)\n",
        "\n",
        "# Mapping categorical values to numerical values\n",
        "data['Merchant_category'] = data['Merchant_category'].map({\n",
        "    'Hotels': 1, 'Fuel': 2, 'Retail': 3, 'Dept stores': 4, 'Rent Payments': 5,\n",
        "    'Utility': 6, 'Wallet load': 7, 'Railways': 8, 'Insurance': 9, 'Medical': 10,\n",
        "    'Cloth stores': 11, 'Education': 12, 'Govt services': 13, 'Home furnishing': 14,\n",
        "    'Misc Services': 15, 'Electronics': 16, 'Restaurants': 17, 'Business Services': 18,\n",
        "    'Transportation services': 19, 'Professional services & memberships': 20,\n",
        "    'Contracted services': 21, 'Food': 22, 'Digital goods': 23, 'Airline': 24,\n",
        "    'Direct marketing': 25, 'Books & stationery': 26, 'Auto': 27, 'Alcohol': 28,\n",
        "    'Music stores': 29, 'Quasi cash': 30, 'Airports': 31\n",
        "})\n",
        "\n",
        "data['merchant_country'] = data['merchant_country'].map({\n",
        "    'IN': 1, 'HK': 2, 'SG': 3, 'NL': 4, 'GB': 5, 'CH': 6, 'SA': 7, 'DE': 8, 'US': 9, 'LK': 10, 'PL': 11,\n",
        "    'AE': 12, 'CA': 13, 'TH': 14, 'LU': 15, 'IE': 16, 'SE': 17, 'OM': 18, 'AU': 19, 'CZ': 20, 'FR': 21, 'NG': 22,\n",
        "    'GH': 23, 'KZ': 24, 'VN': 25, 'AT': 26, 'CY': 27, 'BD': 28, 'CR': 29, 'MV': 30, 'AZ': 31, 'JO': 32, 'GI': 33,\n",
        "    'QA': 34, 'FI': 35, 'IT': 36, 'MY': 37, 'LB': 38, 'TR': 39, 'GR': 40, 'MU': 41, 'MA': 42, 'ES': 43, 'RO': 44,\n",
        "    'NP': 45, 'EG': 46, 'MT': 47, 'ID': 48, 'BR': 49, 'IL': 50, 'BH': 51, 'EE': 52, 'TZ': 53, 'PH': 54, 'ZA': 56,\n",
        "    'AD': 57, 'IS': 58, 'KE': 59, 'DK': 60, 'NZ': 61, 'HU': 62, 'PA': 63, 'LT': 64, 'JP': 65, 'BS': 66, 'SC': 67\n",
        "})\n",
        "\n",
        "data['products'] = data['products'].map({\n",
        "    'Affluent_Card': 1, 'Mass_Card': 2, 'Youth_Card': 3, 'HNI_Card': 4\n",
        "})\n",
        "\n",
        "data.drop([\"merchant_name\", \"married_flag\"], axis=1, inplace=True)\n",
        "data.drop(['Bureau_AL_amt_ever', 'Bureau_BL_amt_ever', 'Bureau_CCOD_amt_ever',\n",
        "           'Bureau_CV_amt_ever', 'Bureau_CD_amt_ever', 'Bureau_EL_amt_ever',\n",
        "           'Bureau_GL_amt_ever', 'Bureau_HL_amt_ever', 'Bureau_PL_amt_ever',\n",
        "           'Bureau_LAP_amt_ever', 'Bureau_TW_amt_ever', 'Bureau_UC_amt_ever',\n",
        "           'Bureau_unsec_amt_ever', 'Bureau_sec_amt_ever', 'Bureau_all_amt_ever',\n",
        "           'Bureau_AL_amt_live', 'Bureau_BL_amt_live', 'Bureau_CCOD_amt_live',\n",
        "           'Bureau_CV_amt_live', 'Bureau_CD_amt_live', 'Bureau_EL_amt_live',\n",
        "           'Bureau_GL_amt_live', 'Bureau_HL_amt_live', 'Bureau_PL_amt_live',\n",
        "           'Bureau_LAP_amt_live', 'Bureau_TW_amt_live', 'Bureau_UC_amt_live',\n",
        "           'Bureau_unsec_amt_live', 'Bureau_sec_amt_live', 'Bureau_all_amt_live'], axis=1, inplace=True)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "model = scaler.fit(data.drop(['target_variable', 'primary_key'], axis=1))\n",
        "scaled_data = model.transform(data.drop(['target_variable', 'primary_key'], axis=1))\n",
        "\n",
        "# PCA for dimensionality reduction on the original data\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(scaled_data)\n",
        "df_pca = pca.transform(scaled_data)\n",
        "df_pca = pd.DataFrame(df_pca, columns=['P1', 'P2', 'P3'])\n",
        "\n",
        "# Modeling on original data\n",
        "y = data['target_variable']\n",
        "\n",
        "# Logistic Regression on original data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_pca, y, test_size=0.33, random_state=101)\n",
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "conf_matrix_log = confusion_matrix(y_test, y_pred_log)\n",
        "print(f\"Logistic Regression Accuracy on Original Data: {accuracy_log}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_log}\")\n",
        "\n",
        "# Support Vector Machine (SVM) on original data\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Classifier Accuracy on Original Data: {accuracy_svm}\")\n",
        "\n",
        "# Undersampling\n",
        "class_count_0, class_count_1 = data['target_variable'].value_counts()\n",
        "class_0 = data[data['target_variable'] == 0]\n",
        "class_1 = data[data['target_variable'] == 1]\n",
        "class_0_undersampled = class_0.sample(class_count_1, random_state=42)\n",
        "undersampled_data = pd.concat([class_0_undersampled, class_1], axis=0).sample(frac=1, random_state=42)\n",
        "X_resampled = undersampled_data.drop('target_variable', axis=1)\n",
        "y_resampled = undersampled_data['target_variable']\n",
        "\n",
        "# Check and handle NaN values in the target variable\n",
        "if y_resampled.isnull().any():\n",
        "    y_resampled = y_resampled.fillna(0)  # Replace NaN with a suitable value (0 in this case)\n",
        "\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# Feature Scaling for undersampled data\n",
        "model2 = scaler.fit(X_resampled.drop(['primary_key'], axis=1))\n",
        "scaled_data2 = model2.transform(X_resampled.drop(['primary_key'], axis=1))\n",
        "\n",
        "# PCA for dimensionality reduction on undersampled data\n",
        "pca2 = PCA(n_components=7)\n",
        "pca2.fit(scaled_data2)\n",
        "df_pca2 = pca2.transform(scaled_data2)\n",
        "df_pca2 = pd.DataFrame(df_pca2, columns=['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7'])\n",
        "df_pca2['target_variable'] = y_resampled\n",
        "df_pca2['primary_key'] = X_resampled['primary_key']\n",
        "# Check and handle NaN values in the undersampled data after PCA\n",
        "df_pca2 = df_pca2.fillna(0)  # Replace NaN with a suitable value (0 in this case)\n",
        "# Split undersampled data into training and testing sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(df_pca2.drop(['target_variable', 'primary_key'], axis=1),\n",
        "                                                        df_pca2['target_variable'], test_size=0.33, random_state=101)\n",
        "\n",
        "# Check and handle NaN values in the training set\n",
        "if y_train2.isnull().any():\n",
        "    y_train2 = y_train2.fillna(0)  # Replace NaN with a suitable value (0 in this case)\n",
        "\n",
        "# Logistic Regression on undersampled data\n",
        "log_model2 = LogisticRegression()\n",
        "log_model2.fit(X_train2, y_train2)\n",
        "y_pred2_log = log_model2.predict(X_test2)\n",
        "accuracy2_log = accuracy_score(y_test2, y_pred2_log)\n",
        "conf_matrix2_log = confusion_matrix(y_test2, y_pred2_log)\n",
        "print(f\"Logistic Regression Accuracy on Undersampled Data: {accuracy2_log}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix2_log}\")\n",
        "\n",
        "# Support Vector Machine (SVM) on undersampled data\n",
        "svm_classifier2 = SVC()\n",
        "svm_classifier2.fit(X_train2, y_train2)\n",
        "y_pred2_svm = svm_classifier2.predict(X_test2)\n",
        "accuracy2_svm = accuracy_score(y_test2, y_pred2_svm)\n",
        "print(f\"SVM Classifier Accuracy on Undersampled Data: {accuracy2_svm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e37va0dWkPr-",
        "outputId": "0feda575-40e5-4ae9-ac4a-a246c9c82ccc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Logistic Regression Accuracy on Original Data: 0.9467272727272728\n",
            "Confusion Matrix:\n",
            "[[15621     0]\n",
            " [  879     0]]\n",
            "SVM Classifier Accuracy on Original Data: 0.9467272727272728\n",
            "1    2620\n",
            "0    2620\n",
            "Name: target_variable, dtype: int64\n",
            "Logistic Regression Accuracy on Undersampled Data: 0.9549132947976878\n",
            "Confusion Matrix:\n",
            "[[1652    1]\n",
            " [  77    0]]\n",
            "SVM Classifier Accuracy on Undersampled Data: 0.9554913294797688\n"
          ]
        }
      ]
    }
  ]
}